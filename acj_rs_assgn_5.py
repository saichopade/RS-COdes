# -*- coding: utf-8 -*-
"""ACJ_RS_Assgn_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pUbIwAxVv1xvrEyvD5Jg1NqiG7PNzwxU
"""

import numpy as np
import pandas as pd

# Step 1: Load the dataset (you would typically use pd.read_csv to load a CSV file,
# but since you provided it in text form, we'll assume it's already loaded into a DataFrame).

# Example DataFrame
data = pd.read_csv('/content/drive/MyDrive/RS Practical 5/Womens Clothing E-Commerce Reviews.csv')

data.dropna(inplace=True)
data.drop(['Unnamed: 0'], axis=1, inplace=True)
data.head()

# Step 2: Create a user-item interaction matrix
# We will assume each user is uniquely represented by their 'Age' and Clothing ID as items.

# Assign each unique clothing item to a column
clothing_items = df['Clothing ID'].unique()
users = df['Age'].unique()

# Create a mapping from clothing items to indices
item_index = {item: idx for idx, item in enumerate(clothing_items)}
user_index = {user: idx for idx, user in enumerate(users)}

# Initialize the user-item interaction matrix (ratings matrix)
user_item_matrix = np.zeros((len(users), len(clothing_items)))

# Populate the user-item matrix with ratings
for _, row in df.iterrows():
    user_idx = user_index[row['Age']]
    item_idx = item_index[row['Clothing ID']]
    user_item_matrix[user_idx, item_idx] = row['Rating']

# Step 3: Matrix factorization using gradient descent
num_users, num_items = user_item_matrix.shape
num_factors = 2  # Latent factors
alpha = 0.01     # Learning rate
num_epochs = 100
lambda_reg = 0.02  # Regularization parameter

# Initialize user and item latent factor matrices with random values
user_factors = np.random.rand(num_users, num_factors)
item_factors = np.random.rand(num_items, num_factors)

# Step 4: Perform matrix factorization using Gradient Descent
for epoch in range(num_epochs):
    for i in range(num_users):
        for j in range(num_items):
            if user_item_matrix[i, j] > 0:  # Only update for existing ratings
                # Calculate the prediction error for user i and item j
                prediction = np.dot(user_factors[i, :], item_factors[j, :].T)
                error = user_item_matrix[i, j] - prediction

                # Update user and item latent factors
                user_factors[i, :] += alpha * (error * item_factors[j, :] - lambda_reg * user_factors[i, :])
                item_factors[j, :] += alpha * (error * user_factors[i, :] - lambda_reg * item_factors[j, :])

    # Calculate the total error every 1000 epochs for monitoring
    if epoch % 10 == 0:
        total_error = 0
        for i in range(num_users):
            for j in range(num_items):
                if user_item_matrix[i, j] > 0:
                    prediction = np.dot(user_factors[i, :], item_factors[j, :].T)
                    total_error += (user_item_matrix[i, j] - prediction) ** 2
        print(f"Epoch {epoch}, Total Error: {total_error/10000:.4f}")

# Step 5: Predict missing values for recommendations
predicted_matrix = np.dot(user_factors, item_factors.T)
print("\nPredicted User-Item Matrix:")
print(predicted_matrix)

# Step 6: Recommend items for a specific user
user_id = 0  # Example user
# Recommend top 2 items that user hasn't interacted with
user_ratings = user_item_matrix[user_id]
predicted_ratings = predicted_matrix[user_id]
recommendations = np.argsort(predicted_ratings - user_ratings)  # Sort by predicted rating

# Only recommend items not yet interacted with
recommended_items = [i for i in recommendations if user_ratings[i] == 0][:2]
print(f"\nTop recommendations for User {user_id} (Age {users[user_id]}): Clothing Item IDs {clothing_items[recommended_items]}")

from google.colab import drive
drive.mount('/content/drive')

!pip install scikit-surprise

import pandas as pd
import numpy as np
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import mean_squared_error

# Assuming 'data' is your original dataframe
df = data.copy()

# Step 1: Adjust index to start from 2 and create the 'user_id' column
df['user_id'] = df.index  # Use the current index as the user_id
df['user_id'] = df['user_id'] + 2  # Adjust index so it starts from 2

# Step 2: Create the user-item interaction matrix (pivot table)
# Using 'user_id' for the index and 'Clothing ID' for the columns
interaction_matrix = df.pivot_table(index='user_id', columns='Clothing ID', values='Rating', fill_value=0)

# Step 3: Apply SVD (Singular Value Decomposition) - Matrix factorization technique
svd = TruncatedSVD(n_components=2)  # Use 2 components, adjust for better performance if needed
latent_matrix = svd.fit_transform(interaction_matrix)

# Step 4: Reconstruct the approximation of the original matrix
approx_matrix = np.dot(latent_matrix, svd.components_)

# Step 5: Calculate RMSE (Root Mean Square Error) for prediction accuracy
# We need to compare the predicted values with actual ratings
# Flatten both the original and approximated matrices for comparison
original_matrix_flat = interaction_matrix.values.flatten()
approx_matrix_flat = approx_matrix.flatten()

# Mask the zero entries in the interaction matrix (because they represent missing values)
mask = original_matrix_flat != 0
rmse = np.sqrt(mean_squared_error(original_matrix_flat[mask], approx_matrix_flat[mask]))
print("Root Mean Square Error (RMSE):", rmse)

# Step 6: Making Recommendations for a specific user
user_id = 5  # Example user, can be any user_id from your dataset
if user_id not in interaction_matrix.index:
    print(f"User {user_id} not found.")
else:
    user_row_index = interaction_matrix.index.get_loc(user_id)  # Find the row index for the user

    # Predict the ratings for items the user hasn't rated yet
    unrated_items = interaction_matrix.iloc[user_row_index] == 0  # Find unrated items
    predicted_ratings = approx_matrix[user_row_index]

    # Get predictions for unrated items
    recommendations = []
    for i, unrated in enumerate(unrated_items):
        if unrated:  # If the item is unrated
            recommendations.append((interaction_matrix.columns[i], predicted_ratings[i]))

    # Sort by predicted rating and recommend the top items
    recommendations.sort(key=lambda x: x[1], reverse=True)
    top_recommendations = recommendations[:3]  # Top 3 recommendations
    print(f"Top Recommendations for User {user_id}: {top_recommendations}")

